多级结构的存储器系统
----------------------

#### 目录
  1. 前言
  1. 存储器系统概述
  1. 高速缓冲存储器 cache
  1. 虚拟存储器
  1. 提高存储器系统性能的可行途径

### 前言
  * 主存储器：又叫内存储器，是传统计算机硬件系统的5大功能部件之一，用于存储处在运行中的程序和相关数据。
  * 现代的计算机系统通常采用三种不同运行原理、性能差异很多的存储介质分别构建__高速缓冲存储器__、__主存储器__、__虚拟存储器__组成三级结构的统一管理、调度的一体化存储器系统。
  * 三级结构的存储器系统围绕主存储器来组织和运行的。


### 存储器系统概述
  * 存储器处于计算机的中心地位，原因：
    1. 当前计算机正在执行的程序和数据都存放在存储器中，CPU直接从存储器中读取指令或存储数据。
    1. 当前计算机系统中输入输出设备的增加，数据传输速度加快，越来越多地采用直接存储访问（DMA）技术和输入输出通道技术，需要在存储器和输入输出设备之间直接传送数据。
    1. 共享存储器的多处理机的出现，需要利用存储器存放共享数据，并实现处理机之间的通讯，更加强了存储器作为计算机中心的地位。
  * 人们常说的“二进制最有效”是指：
    1. 数字信息可以通过区分某些连续的物理量的不同值来存储。但要区分的值越多，相邻的值的差异越小，存储器的可靠性就越低。而二进制数仅需区分两个值，就成了对数据信息进行编码的最可靠的方法。
  * 外存储设备：辅助存储器，通用用来存放贮存中正在使用的程序和数据的副本，使用高速磁盘上的一片存储空间来承担。
  * 衡量存储器系统的三个重要指标：
    1. 容量
    1. 速度
    1. 单位容量的价格
  * 存储器分类：
    * 按存储介质划分：
      1. 磁芯存储器
      1. 半导体存储器 （多数主存使用半导体存储器）
      1. 光电存储器
      1. 磁表面存储器（外存）
      1. 光盘存储器（外存）
    * 按存取方式划分
      1. 随机访问存储器（RAM）：
          * 可以通过指令随机地、个别地对各存储单元进行读写访问的存储器。
          * 一般情况下，访问所需的时间基本固定，与存储单元的地址无关。
          * 目前几乎所有的计算机中 RAM 都是主存储器的主要组成部分
          * 一旦断电，RAM 芯片内的数据全部丢失。
      1. 只读存储器（ROM）：
          * 内容一经写入，仅供读取使用，正常情况下无法再次写入。
          * 断电后保存在内的数据依旧存在。
          * 常用来存放计算机的引导程序、各种字符以及符号的字型库等内容。
          * 它和 RAM 共享计算机主存储器的地址空间，也是主存储器的组成部分。
          * ROM 并非绝对不可写入，随着半导体技术的发展，出现了：
              1. 可编程只读存储器（Programmable ROM, PROM）
              1. 可擦除的可编程只读存储器（Erasible Programmable ROM，EPROM）
    * 按功能效用划分：
      1. 主存储器
          * 存储速度快
          * 容量小
          * 单位价格高
      1. 辅助存储器
          * 主要是磁表面存储器、光盘存储器（如磁盘、磁带和光盘）
          * 容量大
          * 价格低
          * 速度慢
      1. 高速缓冲存储器
          * 速度最快
          * 价格最贵
          * 一般用在 CPU 和主存之间，用来缓存信息。
  * 存储器系统的设计目标
    1. 存储速度
      * CPU 和主存的发展方向不同
    1. CPU着力提高的是其处理速度
    1. 主存着力提高容量，兼顾访问速度
      * 主存速度需要跟上CPU的处理速度，否则无法发挥出其高性能。
    1. 存储容量
    1. 单位价格
  * 存储器系统的解决思路
    * 以上三项指标彼此之间有一定矛盾，在现代的计算机中通常采用由三种运行原理不同、性能差异很大的存储介质分别构建高速缓冲存储器、主存储器和虚拟存储器，再将它们组成三级结构的统一管理、一体化调度的存储器系统。
    * 由高速缓冲存储器缓解主存读写速度慢，不能满足CPU运行速度需要的矛盾；用虚拟存储器更大的存储空间解决主存容量小，存不下更大程序与更多数据的难题。


### 主存储器
  * 主存储器概述
    * 主存储器通过地址总线、数据总线、控制总线与计算机的CPU和外围设备连接在一起。
    * __地址总线__用于选择主存储器的一个存储单元（字，字节），其位数决定了能够访问的存储单元的最大数目，称为最大可寻址空间。例如：20bit 地址可以访问 1MB的存储空间。32bit 地址访问4G空间。
    * 数据总线用于在计算机各部件之间传送数据，数据总线的位数（总线的宽度）与总线时钟频率的乘积，与该总线所支持的最高数据吞吐（输入输出）能力成正比。
    * __控制总线__用于指明总线的工作周期类型和本次读写完成的时刻，总线的工作周期包括主存储器读周期、主存储器写周期、I/O设备读周期、I/O设备写周期，即用不同的总线周期来区分要用哪个部件（主存或I/O设备）和操作的性质（读或写）；此外还有直接存储器访问（DMA）总线周期等。
    * 若计算机中使用不同读写速度的主存储器，在CPU发出读写主存储器的命令后，它无法预知完成的时刻，这是由存储器（或外围设备）本身的运行速度决定的，此时可以让主存储器本身提供读写完成的应答信号（ready），CPU通过检测该信号来得知本次读写完成的时刻。这称为__CPU和主存储器按异步方式运行__。
    * 根据所用的半导体生产工艺却分，存储器芯片又可以分为静态存储器 SRAM 和动态存储器 DRAM 。
      * 静态存储器：读写速度快，生产成本高，通常用来实现容量可以较小，但速度要求较高的高速缓冲存储器。
      * 动态存储器：集成高，生产成本低，被广泛应用于实现对容量要求较高的主存储器。
      * 静态存储器和动态存储器的主要区别：

    <table>
      <tr>
      <td>性能</td><td>SRAM</td><td>DRAM</td>
      </tr>
      <tr>
      <td>存储信息</td><td>触发器</td><td>电容</td>
      </tr>
      <tr>
      <td>破坏性读出</td><td>否</td><td>是</td>
      </tr>
      <tr>
      <td>需要刷新</td><td>否</td><td>是</td>
      </tr>
      <tr>
      <td>行列地址</td><td>同时送</td><td>分两次送</td>
      </tr>
      <tr>
      <td>运行速度</td><td>快</td><td>慢</td>
      </tr>
      <tr>
      <td>集成度</td><td>低</td><td>高</td>
      </tr>
      <tr>
      <td>发热量</td><td>大</td><td>小</td>
      </tr>
      <tr>
      <td>存储成本</td><td>高</td><td>低</td>
      </tr>
    </table>

  * 动态存储器芯片的存储原理和读写过程
    * 读操作过程会使电容中原来存储的电荷信息丢失，所以这就是常说的说动态存储器的破坏性读取。
    * 动态存储器的读写周期必要它的数据读出时间长很多。
    * 向动态存储器的存储单元提供地址时，是按先送行地址，然后再送列地址的次序分别完成的，由存储器芯片本身的时序信号控制完成。
    * 另外需要分别处理行、列地址的原因还有：动态存储器需要不断地刷新，补充电容电荷，保持数据。这种刷新操作是以行为单位执行的，此时只需提供行地址即可，与列地址无关。
    * 刷新操作采用读操作实现。
  * 静态存储器的存储原理和内部结构
    * 静态存储器（SRAM）是用触发器线路记忆与读写数据的，通常用6个MOS管组成。
    * 读取数据时，数据不会发生变化。
  * 主存储器实现与应用的技术
    1. 动态存储器的快速读写技术
        * 快速页式工作技术：当连续读写属于一行的多个列中的数据时，其行地址只需在第一次读写时送入(锁存)，之后保持不变；读写其它列数据时，只需送入列地址即可。
        * EDO技术（extended data out）：在输出部分增加数据锁存线路，延长输出数据的有效时间，即使地址信号发生了变化，仍能取得正确的读出数据
    1. 主存储器的并行读写技术
      * 主存储器的并行读写是只在主存储器的一个工作周期或略多一点的时间内可以读出多个主存字所采用的技术。在静态和动态存储器中均可使用。有两种可行的方案：
      1. 一体多字技术：
          * 加宽每个主存单元的宽度，使每个主存单元同时存储几个主存字，读出时，同时将几个字的内容读出，放到寄存器中。所以读出一个
        主存字的平均读出时间变为原来的几分之一。
          * 缺点：每次读出的几个主存字必须首先保存在一个位数足够长的寄存器中，等待数据总线分几次被传送走。
      1. 多体交叉编址技术：
        * 把主存储器分成几个能独立读写的字长为一个主存字的主体，这样就可以按读写需要情况分别对每个存储体执行读写，通过合理的组织方式，使几个存储体协同工作，从而提供比单个存储体更高的读写速度。合理地对这几个存储体进行组织，涉及以下两个问题：
          1. 如何对存储体执行读写？
              1. 在同一个读写周期同时启动所有存储体的度操作或写操作，这与前面讲的一体多字方案类似。
              1. 使这些存储体顺序地轮流启动各自的读写周期，理论上能达到的最高读写速度是在一个存储体的读写周期内能启动每一个存储体的读写操作，即启动相邻两个存储体的最小时间间隔要小于或等于一个读写周期除以存储体的个数。
            这方案的优点是依次读出来的每一个存储字可以直接通过数据总线依次地传送走，而不必设置专门的数据缓冲寄存器。
          1. 如何分配这些存储体各自工作的地址范围。
              * 合理的方案是交叉编址，即把__连续地址的几个主存字一次分配在不同的存储体中__因为程序运行的局部性特性已表明，程序运行过程中，在短时间内读写地址相邻的贮存自的概率更大。假定有M个存储体，每个存储体的容量为L，则第m个存储体中的主存字的地址应为：`m × j + i  其中  j = 0, 1, 2, ..., L-1; i = 0, 1, 2, ..., M-1`
              在这种编址方式中，地址及其存期送到主存储器的地址的低几位用于区分读写哪个存储体，其余高位部分送到每个存储体，用于区分读写每个存储体的哪一个存储字。
    1. 关于对成组数据传送的支持
        * 成组传送数据的方式（burst mode）是指用于提高数据总线上的数据传送能力的一种技术，即通过地址总线传送一次地址后，能连续在数据总线上传送多个（一组）数据，而不像正常总线工作方式（normal mode）那样，每传送一次数据，总要两段时间（先送一次地址，地址时间；再进行一次数据传送，数据时间）。
        * 在成组传送方式下，要传送N个数据，就可以仅用 N+1 个总线时钟，而不再是用 2 × N 个总线时钟，因此使总线上的数据输入输出尖峰值提高了一倍。


### 高速缓冲存储器 cache
  * 为了兼顾速度、容量、单位价格，使主存储器系统达到最佳性能，将高速缓冲存储器、内存、虚存三级存储器统一管理、一体化调度
  * 三级结构主存储器系统，从功能上看，是完成主存储器的所有功能；高速缓冲存储器是为了加快主存读写速度而设置，虚存是为了增加主存容量而设置。
  1. 层次存储器系统的运行原理和必须遵从的原则
    * 层次存储结构能同时满足CPU对存储系统在速度、容量两方面的要求，因为计算机策划年供需具有__局部性__。
    * 程序局部性原理：CPU 对存储器中的程序和数据的访问，在一小段时间内，总是集中在一小块存储空间。包含三个方面的含义：
        1. 时间方面：在一小段时间内，最近被访问过的程序和数据很可能被再次访问。
        1. 空间方面：折子最近被访问过的程序和数据往往集中在一小片存储区域中。
        1. 指令执行顺序方面：指令顺序执行比转移执行的可能要大。（约5：1）
    * 根据程序局部性原理
        * 把CPU近期频繁访问的程序和数据存放在速度基本上能与其匹配的静态存储器中，这样CPU再次访问这些程序和数据时，就不必访问速度较慢的动态存储器，可以从高速的静态存储器中直接得到。
        * 在较长的时间内用不到的程序和数据就可以放在由辅助存储区构成的虚拟存储器中，它们需要满足如下两个原则，使CPU和其他总线主设备能够正确地对他们进行访问：
            1. 一致性原则。同一个信息会同时存放在几个级别的存储器中。这些信息在不同级别存储器中必须保持相同的值。
            1. 包含性原则。处于内层(更靠近CPU)存储器中的信息，一定被包含在各外层的存储器中。即内层存储器中的全部信息一定是各外层存储器中所存信息中一小部分的副本，这是保证程序正常运行、信息共享提高系统资源利用率的必需的。
  1. 高速缓冲存储器 cache 的工作原理和组织
    1. cache 的工作原理
        * 为保证速度，cache的全部功能均由硬件实现，并且对程序员透明。
        * cache 由 cache块组成，每块有K个字。假设主存共有 2^n 个字，每个字的地址为 n 位。为使主存和 cache 块相互映射可以把主存也划分为K个字的块，也就是说，共有 M=2^n/K 块。
        * 当CPU需要主存中的一个字时，若该字不在 cache 中，则包含该字的主存块将被装入到 cache 中的一块中。
        * cache中的块永远不可能对应主存中的某一个固定的块，每个 cache 块还必须有一个标志，来表示现在在 cache 中的块是主存中的哪一个块。这个标志一般是主存地址的一部分。
        * CPU 读 cache 原理：
            1. CPU 首先给出要访问的存储字的地址 RA，同时送给 cache 和主存储器。
            1. 如果该字已在 cache 中（可以通过对地址 RA 和标志位进行比较来判断），则 cache 中的内容被送到CPU中。
            1. 如果不在 cache 中，就要从主存储器中将包含该字的存储块装入到 cache 的一个字块中，并将该字的内容送给 CPU，同时设置该块的标志位。
    1. cache 的组织
      * cache 的块比内存中的块少得多。所以需要一个算法来确定cache中块与内存中块的对应关系，以及记录cache正在被哪个内存块占用。
      * 目前有三种映像方式可以解决这个问题：
          1. 直接映像
          1. 全相连映像
          1. 组相连映像
      * 下面介绍以上三种方式：
         * 设定下面相关参数：
            * cache 的容量为 64KB。
            * cache 和主存中的块的大小都为4B，cache中共有2^14个2B大小的块。
            * 主存容量为16MB，其地址为24位（2^24B = 16MB），即主存中共有2^22个4B的块。
         * 直接映像方式：
            * 最简单的主存和cache映像方式，表示如下： 
              `i = j mod m`
            其中 i 为 cache 中的块号，j为主存中的块号，m为cache中的总块数。
            * 直接映像方式的有点是它的实现十分简单，只需利用主存中地址的某些部分进制直接判断。可以把主存地址分为三部分，最低的w位确定了主存地址中某块内的单个字节，其他的s位标明的是2^s块中的一块。cache的地址逻辑把这s位分解为两部分，即r位的cache块号和最高的s-r位的标记。cache 中一共包含有 m = 2^r 块。将地址如此分解后，主存中的块和cache中的块对应关系如下：
            <table>
            <tr>
              <td>cache 块号</td>
              <td>可能对应的主存块号</td>
            </tr>
            <tr>
              <td>0</td>
              <td>0, m, …, 2^s -m</td>
            </tr>
            <tr>
              <td>1</td>
              <td>1, m+1, …, 2^s-m+1</td>
            </tr>
            <tr>
              <td>…</td>
              <td>…</td>
            </tr>
            <tr>
              <td>m-1</td>
              <td>m-1, 2m-1, 2^s-1</td>
            </tr>
            </table>
          * 用这种方式，主存中的块只可能对应 cache 中确定的块，并且一旦主存中的某块被装入到 cache 中的块中，cache还必须将该块标记出对应的是主存中的哪一块，最高的s-r个标记位可以达到这个目的。
          * 缺点：由于主存中的块只能对应cache中唯一的一块，如果CPU确实要同时访问对应于同一cache块的两个不同的主存块，将引起两块在cache中频繁地切换，这势必要降低cache的性能。
        * 全相连映像
          * 全相连映像方式允许主存中的块可以和 cache 中的任意块相对应。为达到这个目的，cache的比较电路把地址分为标记和块内地址两部分，标记部分应该能唯一标识目前在cache中的是主存中哪一块。为确定主存块是否在cache中，比较电路必须对cache中所有块的标志位进行比较。
          * 使用全相连映像方式实现，主存地址将被分解为22位的标志位和2位的块内地址。标志位和数据位组合在一起形成cache字，比较电路需要将地址中的标记部分和全部 cache 块的标志为孩进行比较，才能得到该地址是否已经在 cache 中的结果。
          * 缺点是，比较电路复杂，实现的成本过高。
        * 多路组相连映像
          * 多路组相连映像是对直接映像和全相连映像的折中，以弥补他们的不足。
          * 它将cache分为v组，每组包含有k个块。主存块和cache块的对应关系可以用如下的表达式表示：
            `m = v × k`, `i = j mod v`
          其中，i 为 cache 的组号，j 为主存中的块号，m为cache中的块数。
          * 以这种映像方式，对于主存中任意一块 j 在cache中可以确定唯一的一个组 i 来存放，但具体放在第 i 组中的那一块则是完全随意的，这种方式通过直接映像方式来确定组号，在一个组内，则通过全相连映像方式来确定 cache 中的块号。
          * 采用这种方式，对cache的地址解释电路来说，只需要将主存地址分解为三个部分，即（s-d）位的标记位 d 为的组地址和 w 位的组内地址。显然，d 位的组地址可以确定 v = 2^d 个组，而 s 位地址可以表示主存中共有 2^s 块。为确定主存中某块是否存在cache的某组中，只需比较s-d位标记位，看它是否已被装入到cache中；为确定它在cache 中的具体块，则还需与组内的k块逐一进行比较。
          * 先假定每组中有两个cache块，也就是采用两路组相连方式，即将2^14块的cache分为2^13组。13位组成的组号可以唯一确定cache中的一组。将主存块号对2^13取模，可以确定这些块和cache中组的对应关系。
          * 当v=m, k=1 时，多路组相连方式即为直接映像方式；而当 v=1, k=m 时，它又变成了全相连映像方式。其中两路组相连方式（v=m/2, k=2）最常用。
          * 多路组相连映像的对应关系比直接映像相连更灵活，提高了cache 的命中率，实现成本又比全相连映像方式更低。
    1. cache 有关的问题
      1. 替换算法
        * cache 中的块比内存中的块少得多，当新的一个主存块需要装入cache时，就需要决定哪一个cache块被替换。对于__直接映像方式__，一个主存块仅和唯一的一个cache块对应，也就不存在什么选择。而对于全相连映像方式和组相连方式，由于一个主存块可以装入到不同的cache块中，就需要有一个替换算法来决定哪一个cache块被替换出去。为了保证cache的性能，这个算法还应该用硬件实现。
        * 两种替换算法：
          1. 最近最少使用法，LRU（least-recently-used），即替换出组内没有被访问的时间最长的一个cache块。
            * 算法实现：在组内的两块cache块中分别增加一个使用位，初始值均为0。当某块中的内容被访问时，将它的使用位置为1，而将另外一块的使用位置为0。这样当有新的主存块要装入这个组的时候，只需要将使用位为0的那块替换出去。
          1. 先进先出法，FIFO（first-in-first-out），即替换出最先装入到cache中的那个主存块，这可以用一个循环电路来实现。
          1. 最少使用法，LFU（least-frequently-used），即将使用次数最小的一个cache块替换出去，它需要在每个cache块上加一个计数器来记录CPU对它的访问次数。
          1. 随机替换算法，随机找一块替换出去。
      1. 写cache策划
        * cache 与外层存储器的数据同步（一致性原则）将导致一个问题。当计算机存在多个CPU或外设将内存的数据进行修改的时候，cache 与内存储器里的数据不能及时同步。
        * 针对上述问题，常用两种策略解决：
          1. 写直达（write through）：写了 cache 之后立即修改内存中对应的值。
            * 优点：处理简单，数据一致性容易保证。
            * 缺点：配置cache对写操作来说不但没有提高性能，甚至会降低系统的速度。在没有其他CPU、外设去读这些被重写过的主存单元时，这些写主存的操作是徒劳无功的，他只起到了减慢系统速度的作用。
          1. 拖后写（write back）改善写直达的策略，在写完 cache 之后，不直接去改写相应的主存单元的内容，而把这些主存单元的地址登记到cache控制器中的一张列表中，若有另外的CPU或外设发出读主存的请求，并将主存地址送到地址总线上时们首先将他们要读的地址与原来保存在cache的那张列表中的全部地址内容相比较这一操作称为总线监听。当地址出现在列表中，首先发出命令停止该读操作，再把保存在cache中的正确数据通过一个写主存周期将其写入相应的主存单元，接下来再启动刚停下来的读操作，则CPU或外设读出的一定是正确的数据。
            * 优点：确保每一次写主存都是必须完成的那一部分，而不全出现徒劳的写操作，系统性能高。
            * 缺点：实现复杂，成本高。
      1. cache命中率
        * cache命中率与cache 的容量相关。但cache容量达到一定大小的时候，对cache的命中率影响不明显。而组成cache 必须是价格昂贵的SRAM，所以cache容量一般控制在合理的范围内。
        * 影响cache命中率的因素很多，包括cache容量，cache块的大小，cache的组织方式以及cache的数量等。
          * 容量：cache与主存交换数据是以块为单位的，当CPU要装入一个主存字到cache内时，不仅仅是装入它需要的这一个字，而是包括之相邻的其他字的整体主存块。当容量增加，根据局部性原理，cache的命中率就会增加。但是容量不是越大越好，容量太大，需要装入的数据也多，转入所需要的时间也将增加，影响系统性能。一般来说cache块的容量在4~32个字节范围之间比较适宜。
          * 组织结构：直接映像方式命中率比较低，全相连映像方式命中率比较高。但从性价比考虑，2路/4路组相连方式最适合。
        * 多级高速缓冲器
          * 如果计算机中存在多个cache ，称为多级cache结构。
          * 最接近CPU的cache叫做第一级cache，容量较小。
          * 在第一级cache之外是第二级cache，容量比第一级cache大的多，当CPU在第一级cache中查找不到需要的数据的时候，会到第二级cache中查找。
          * 依照第二级cache的思路可以增加第三级cache。
          * 命中率： 如果第一级，第二级cache的命中率为 90%，则他们合起来之后的命中率是 `1 - (1 - 90%) × (1-90%) = 99%`，而不是81%。
      1. cache 的接入法
        1. look-aside cache architecture，将cache系统如同一个接口卡一样地接到计算机主总线上。
          * 优点：实现简单，成本低。
          * 缺点：当CPU与cache交换信息时，它占用了总线，使接在总线上的其他 bus master 不能与主存交换信息。
        1. look-through cache architecture，把原来的处理机总线打断，用cache把原来的总线隔断为两个部分。
          * 优点：提高了总线上并发操作的可能性，但CPU与cache交换信息时，bus master 可以同时与主存交换信息，有利于提高计算机整体性能。
          * 缺点：设计复杂，成本较高，而且在读cache不命中的情况下，读主存用的时间会更多一点，它在读cache失败之后才启动读主存。


### 虚拟存储器
  1. 概念
    * 虚拟存储器使程序员在不增加主存储器容量的情况下，拥有比主存储器容量大得多的地址空间。
    * 虚拟存储器的工作原理和cache十分类似。将主内存与虚存划分成很多个小块，所有程序和数据开始都存放在虚存中。当CPU需要访问才转入内存中，内存装满，再通过一定的算法替换掉内存中的数据。
    * 虚拟存储器与cache的重要区别：
      1. 交换频率：cache与内存交换的频率高，而内存与虚拟存储器交换的频率较低。
      1. 交换的数据量：cache与内存交换的数据量较少，而内存与虚存交换的数据量较大。
      1. 性能、效率：cache 和 虚存的主要作用不同，导致两者的效率相差很大，cache 的效率较高，虚存的效率低。
      1. 实现：cache 和虚存的实现不同，cache通过硬件实现，而虚存通过操作系统（软件）实现。（也一定表明效率的不同）。
      1. 块容量：cache 为了使硬件实现方便，每块容量大小是固定的；而虚拟存储器基本的信息交换单位有几种不同的方案：段、页和段页。
    * 三种方案：
      1. 段：
        * 段是利用程序的模块化性质，将程序按照逻辑结果划分成多个相对独立的部分，如过程、函数、数据表等。段作为独立的逻辑单元可以被其他程序段调用，这样可以形成段间连接，产生规模较大的程序。因此从逻辑上讲，将短作为基本信息单元在辅存和主存之间进行数据交换是比较合理的。操作系统用段表来致命各段在主存中的位置。每段都有段的名字、段起点和段长等，甚至段表本身也可以是一个独立的段。、
        * 采用对主存分段的方式来管理主存，具有以下的有点：
          1. 可以简化动态存储分配的操作。
              * 当程序员无法实现定义某些数据结构的大小的时候，可以给这个数据结构单独定义一个段，交操作系统系管理，由操作系统给它动态分配空间。
          1. 易于独立修改和编译，不用将整个程序重新编译。
          1. 易于实现不同进行之间对主存的共享。
            * 可以将一些公用函数和数据放在共享段中供其他进行访问。
          1. 易于对段进行保护。
            * 由于段有独立的程序块或者数据组成，程序员或者系统管理员可以方便地对其访问权限进行控制。
      1. 页：
        * 页式管理，基本的信息传递单位是长度一定的页，主存也按照这个长度划分成等长的块，也就是常说的页面。
        * 优点：
          1. 操作系统对页的管理通过页表完成，由于页面的起点和终点是地址固定的，所以页表中不再需要存放页长，方便页面维护。
          1. 新页调入主存也容易，只要有空白页或替换出一页即可。
          1. 低浪费，可能出现碎片的地方只可能在程序的最后一页，这种浪费比段式管理要少得多。
        * 缺点：
          1. 页不是逻辑上的独立实体，所以对页面进行修改、保护和共享都不如段式管理方便。
      1. 段页式：
        * 对段式管理和页式管理的综合。
        * 实现思路：将程序按逻辑结果分段，利用对它们进行共享、保护和管理，然后在每个程序段内部进行分页，虚存和实存之间按页交换数据。
  1. 段式虚拟存储器
    * 操作系统通过段表对段进行管理，段表本身也是一个段，其起始地址由段表基地址寄存器给出。
    * 使用虚拟地址空间编程时，虚拟地址由段号和段内地址组成。
    * 操作系统获得实际地址的过程：
      1. 操作系统得到请求后，将逻辑地址中的段号与段表基地址相加，得到的和作为地址
      1. 找到段表的一个表项，检查该表项中的装入位，若其内容为1，表示该段已经调入主存
      1. 从表项中取段的起始地址与逻辑地址中的段内地址相加，就得到一个数据在主内中的实际地址。
      1. 若表项中装入位得值为0，即该段尚未调入主存，则操作系统负责首先把该段从磁盘转入主存，并相应修改段表中该表项的内容
      1. 按照第二、三步骤转换实际地址。
    * 在实际实现中，还设立一个段地址寄存器，当一段刚投入运行过程时，还会把这一段在主存中的起始地址首先写入段地址寄存器，在该段其后的运行过程中，就不必再查段表，而用段地址寄存器的内容直接与段内地址相加，以便快读得到一个数据在内存中的实际地址，提高系统的运行性能。
  1. 页式虚拟存储器
    * 以页为基本单位进行虚存和主存之间的数据交换。页的长度是固定的，并且为2的整数幂个字，所以页的起点和终点都会落在低位字段为0的地址上，这样可以把虚拟地址分为两个字段，高位字段为虚页号，低位字段为页内地址。
    * 虚拟地址到主存的实际地址的转换是通过页表来实现的。页表的每条记录表示一个虚页号和他对应的实页号，把实页号和虚拟地址中的地位字段拼接在一起，就是所要访问的字的实际主存地址。
    * 页表记录会记录：
      1. 虚页号
      1. 实页号
      1. 有效位（表示该页是否已经装入主存中）
      1. 修改位（表示该页面的内容是否被修改）
      1. 替换控制位（用来指出需要替换的页）
    * 页式管理的问题：
      * 虚拟存储器页表保存在主存储器中，当CPU给出虚地址后，需要进行两次主存访问才能获取到数据。这样导致主存的访问效率降了一倍。如果被访问的页不在主存中，还需要进行替换、修改操作，效率就更低了。
      * 解决：
        * 借鉴 cache 的方法，将页表活跃的部分存放在高速存储器中，组成快表，对快表的查找算法也用硬件实现。块表又称转换旁路缓冲器（Translation Lookaside Buffer，TLB）。它比页面要小的多，一般在16~64行之间，快表是页表（页表也叫慢表）的小的副本。
        * 查表操作过程：
          1. 查表时候，同时查快表和慢表。
          1. 若在快表中找到该虚页号，终止慢表的查询操作。
          1. 否则用一个访问主存的时间去查慢表，从中找到对应的实页号，并根据替换算法将这页的虚页号和实页号送到快表，替换其中的一行内容。
  1. 段页式虚拟存储器
    * 段式和页式的结合。将程序按照逻辑单位分段以后，在把每段划分成固定大小的页。
    * 操作系统按页面对程序进行调入调出主存操作，但同时按段对程序进行保护和共享。
    * 同时具有段式管理和页式管理的优点，但缺点是在地址变换过程中需要多次查表，因为它的程序需要通过一个段表和一组页表来进行地址变换。
    * 段表中的每条记录对应一个程序段，包括该段的起始地址和该段的控制保护信息，而由页表来指明该段哥页在主存中的位置以及是否已转入、已修改等标志，是目前使用广泛的存储管理方式。


### 提高存储器系统性能的可行途径
  1. 提高存储器芯片本身的读写速度
    1. 增强型动态存储器芯片，（enhanced DRAM，EDRAM）在DRAM中集成一个容量小一点的SRAM。SRAM作为cache存放最后一次读操作涉及到的完整一行的内容。如果接下来的读取仍是这一行，就可以并行执行。
    1. 高速缓冲存储器动态存储器，（cache DRAM，CDRAM）。集成的 DRAM 容量更大一些。用法：
      1. 以芯片内部的cache方式运行。
      1. 把 SRAM作为块数据输出的缓冲区域使用。
    1. 同步动态存储器，（synchronous DRAM， SDRAM），实现同步的动态存储器芯片，取消CPU访问存储器时的等待状态，使得CPU可以在总线的最高速度同步地与存储器交换信息。
  1. 改进芯片之间的组合与结构关系
  1. 多端口的存储器芯片、
